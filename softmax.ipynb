{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.364589\n",
      "soln loss: 2.364589\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive_soln\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('soln loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n",
    "We expect all the scores to be evenly distributed, so e^S_yi = e^S_j for all j. This leads to the arguemnt to the\n",
    "log function to be ~1/N where N is number of classes. In this case, N = 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.139897 analytic: 1.018378, relative error: 5.630401e-02\n",
      "numerical: 0.480025 analytic: 0.397304, relative error: 9.428724e-02\n",
      "numerical: 1.843396 analytic: 1.611966, relative error: 6.697709e-02\n",
      "numerical: 1.602670 analytic: 1.408876, relative error: 6.435060e-02\n",
      "numerical: 2.410375 analytic: 2.352210, relative error: 1.221275e-02\n",
      "numerical: 1.778426 analytic: 1.750881, relative error: 7.804798e-03\n",
      "numerical: -0.847236 analytic: -0.756367, relative error: 5.666610e-02\n",
      "numerical: 0.167139 analytic: 0.114843, relative error: 1.854565e-01\n",
      "numerical: 0.501509 analytic: 0.412622, relative error: 9.723618e-02\n",
      "numerical: 3.789519 analytic: 3.362074, relative error: 5.976918e-02\n",
      "add regularization\n",
      "numerical: 1.031207 analytic: 0.957646, relative error: 3.698661e-02\n",
      "numerical: 0.473369 analytic: 0.262744, relative error: 2.861307e-01\n",
      "numerical: 1.734502 analytic: 1.504525, relative error: 7.100194e-02\n",
      "numerical: 2.616403 analytic: 2.378853, relative error: 4.755522e-02\n",
      "numerical: -0.709334 analytic: -0.655566, relative error: 3.939331e-02\n",
      "numerical: 4.785906 analytic: 4.333589, relative error: 4.959893e-02\n",
      "numerical: 0.650029 analytic: 0.572261, relative error: 6.362491e-02\n",
      "numerical: 0.484308 analytic: 0.579868, relative error: 8.979710e-02\n",
      "numerical: 0.193088 analytic: 0.285135, relative error: 1.924772e-01\n",
      "numerical: 1.502650 analytic: 1.377103, relative error: 4.359636e-02\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive_soln(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive_soln(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "print(\"add regularization\")\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive_soln(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive_soln(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.184243 analytic: 2.184243, relative error: 1.665854e-09\n",
      "numerical: -1.851608 analytic: -1.851608, relative error: 2.094729e-08\n",
      "numerical: 3.647062 analytic: 3.647062, relative error: 9.040043e-09\n",
      "numerical: -2.040290 analytic: -2.040290, relative error: 4.699818e-10\n",
      "numerical: -1.803662 analytic: -1.803662, relative error: 4.695374e-09\n",
      "numerical: 0.032086 analytic: 0.032086, relative error: 5.752071e-07\n",
      "numerical: -1.231095 analytic: -1.231095, relative error: 7.840559e-08\n",
      "numerical: -0.471769 analytic: -0.471769, relative error: 6.511260e-09\n",
      "numerical: 0.210547 analytic: 0.210547, relative error: 4.383300e-09\n",
      "numerical: -0.046948 analytic: -0.046948, relative error: 3.582322e-08\n",
      "add regularization\n",
      "numerical: 0.564775 analytic: 0.564775, relative error: 9.271522e-09\n",
      "numerical: -0.747938 analytic: -0.747938, relative error: 1.641424e-08\n",
      "numerical: 1.806685 analytic: 1.806685, relative error: 3.039931e-08\n",
      "numerical: -0.083390 analytic: -0.083390, relative error: 2.026959e-07\n",
      "numerical: 0.611957 analytic: 0.611956, relative error: 4.401558e-08\n",
      "numerical: -3.853691 analytic: -3.853691, relative error: 1.325636e-08\n",
      "numerical: -0.671497 analytic: -0.671497, relative error: 5.847582e-08\n",
      "numerical: 2.139721 analytic: 2.139720, relative error: 9.604964e-09\n",
      "numerical: -3.459231 analytic: -3.459231, relative error: 4.777124e-09\n",
      "numerical: 0.747388 analytic: 0.747388, relative error: 2.127963e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "print(\"add regularization\")\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.366329e+00 computed in 0.266585s\n",
      "S\n",
      "(500, 10)\n",
      "[[-0.48806417 -0.16424376  0.00749817 ...,  0.25395822  0.08129688\n",
      "   0.34630356]\n",
      " [-0.50933788  0.46124392 -0.61024699 ...,  0.49642472  0.09487644\n",
      "   0.10893162]\n",
      " [-0.83931913  0.50259181 -0.59721728 ...,  0.74869029 -0.19991291\n",
      "   0.56288048]\n",
      " ..., \n",
      " [ 0.6911079  -0.7301356   0.10397017 ..., -0.94460257 -0.15289159\n",
      "  -0.15547923]\n",
      " [-0.03206701 -0.23722788 -0.09860362 ..., -0.22467863 -0.03093766\n",
      "   0.21300962]\n",
      " [-0.36451755  0.32081483 -0.19879968 ...,  0.21101502 -0.06534451\n",
      "   0.07764532]]\n",
      "expS\n",
      "(500, 10)\n",
      "[[ 0.61381348  0.84853515  1.00752635 ...,  1.28911794  1.08469287\n",
      "   1.41383174]\n",
      " [ 0.60089331  1.58604566  0.54321668 ...,  1.64283716  1.09952299\n",
      "   1.11508609]\n",
      " [ 0.43200456  1.65299999  0.55034095 ...,  2.11422918  0.81880206\n",
      "   1.75572256]\n",
      " ..., \n",
      " [ 1.9959256   0.48184365  1.10956735 ...,  0.38883407  0.85822276\n",
      "   0.85600485]\n",
      " [ 0.96844168  0.78881151  0.90610179 ...,  0.79877288  0.96953601\n",
      "   1.23739655]\n",
      " [ 0.69453165  1.37825035  0.81971408 ...,  1.23493091  0.93674469\n",
      "   1.08073928]]\n",
      "SumExpS\n",
      "(500, 1)\n",
      "[[  9.82156243]\n",
      " [  9.80537613]\n",
      " [ 10.14179637]\n",
      " [  8.99596715]\n",
      " [ 10.12334567]\n",
      " [  9.28130224]\n",
      " [ 11.89447255]\n",
      " [  9.40651915]\n",
      " [ 11.9625868 ]\n",
      " [ 12.10366858]\n",
      " [ 10.10112942]\n",
      " [  9.82515796]\n",
      " [  9.21317508]\n",
      " [  9.16259406]\n",
      " [ 10.15315854]\n",
      " [ 10.13318278]\n",
      " [ 12.18011656]\n",
      " [ 11.76254046]\n",
      " [ 12.44925338]\n",
      " [ 10.77238219]\n",
      " [ 10.92754791]\n",
      " [ 11.0563683 ]\n",
      " [ 15.19824939]\n",
      " [ 10.38330815]\n",
      " [ 11.14758463]\n",
      " [  9.7998929 ]\n",
      " [ 11.59496865]\n",
      " [  8.50288878]\n",
      " [ 10.62841764]\n",
      " [  9.93442564]\n",
      " [ 10.90151121]\n",
      " [ 12.30248057]\n",
      " [ 10.70678733]\n",
      " [ 12.84367784]\n",
      " [  8.92012106]\n",
      " [ 11.05078156]\n",
      " [  9.17135707]\n",
      " [  9.94477198]\n",
      " [  9.66064142]\n",
      " [  9.97759909]\n",
      " [  9.88722412]\n",
      " [  8.87022257]\n",
      " [ 10.21546753]\n",
      " [ 10.36413955]\n",
      " [  9.11459264]\n",
      " [ 10.45508886]\n",
      " [ 10.50881919]\n",
      " [ 10.32538336]\n",
      " [ 13.20049803]\n",
      " [  9.36650992]\n",
      " [  9.01113455]\n",
      " [ 11.92257892]\n",
      " [ 12.924645  ]\n",
      " [ 10.92544939]\n",
      " [  9.98339187]\n",
      " [ 11.63142087]\n",
      " [ 11.20104495]\n",
      " [  9.83204706]\n",
      " [ 14.79772464]\n",
      " [ 10.52591511]\n",
      " [  9.9647382 ]\n",
      " [  9.90723339]\n",
      " [  9.9664084 ]\n",
      " [ 11.19596328]\n",
      " [  8.93559153]\n",
      " [ 10.88527621]\n",
      " [  9.52056666]\n",
      " [  8.75266381]\n",
      " [ 14.82284641]\n",
      " [ 12.04313566]\n",
      " [  9.54375345]\n",
      " [ 11.92353041]\n",
      " [  9.72609121]\n",
      " [ 10.35110061]\n",
      " [  8.38694273]\n",
      " [  9.366976  ]\n",
      " [  9.28858569]\n",
      " [ 11.45005449]\n",
      " [  8.72808604]\n",
      " [ 11.09918828]\n",
      " [ 10.51362349]\n",
      " [ 11.16297954]\n",
      " [ 11.91938611]\n",
      " [  9.68477891]\n",
      " [  8.91588249]\n",
      " [ 11.96319678]\n",
      " [  9.75737824]\n",
      " [  8.81689064]\n",
      " [  8.88869369]\n",
      " [ 12.51291298]\n",
      " [  9.03360801]\n",
      " [ 10.44619687]\n",
      " [ 10.35870092]\n",
      " [ 10.03316207]\n",
      " [ 11.56396406]\n",
      " [ 10.14561691]\n",
      " [ 10.32469088]\n",
      " [  9.18659595]\n",
      " [  8.46437843]\n",
      " [  9.66983477]\n",
      " [ 10.15129155]\n",
      " [  9.4290687 ]\n",
      " [ 11.13803665]\n",
      " [ 12.19384024]\n",
      " [  9.4475499 ]\n",
      " [ 11.7819283 ]\n",
      " [  8.8299527 ]\n",
      " [  8.91901379]\n",
      " [ 10.6729561 ]\n",
      " [ 10.02792899]\n",
      " [  9.35831831]\n",
      " [  9.96305693]\n",
      " [ 12.70351061]\n",
      " [  9.91293488]\n",
      " [  9.91140693]\n",
      " [  8.58709852]\n",
      " [ 10.62410226]\n",
      " [  8.99930787]\n",
      " [ 12.77249637]\n",
      " [  9.40701026]\n",
      " [  9.60654819]\n",
      " [  9.79658426]\n",
      " [  9.82863218]\n",
      " [ 10.81420133]\n",
      " [ 11.58329152]\n",
      " [  9.04286491]\n",
      " [  9.59246536]\n",
      " [ 13.34313871]\n",
      " [ 11.18642628]\n",
      " [ 11.61954434]\n",
      " [ 11.94792743]\n",
      " [  8.91028625]\n",
      " [ 10.10221144]\n",
      " [  9.67025892]\n",
      " [  9.04027962]\n",
      " [ 12.5471896 ]\n",
      " [ 10.39708456]\n",
      " [ 10.63720207]\n",
      " [  9.67264799]\n",
      " [ 10.72731898]\n",
      " [  9.63023021]\n",
      " [ 11.93173209]\n",
      " [ 11.11269096]\n",
      " [ 10.46777953]\n",
      " [  9.70598076]\n",
      " [ 13.24417289]\n",
      " [  9.63245544]\n",
      " [  9.9603814 ]\n",
      " [ 11.4148471 ]\n",
      " [  9.53509553]\n",
      " [ 11.12346776]\n",
      " [ 10.80312993]\n",
      " [ 10.07800843]\n",
      " [ 11.29672211]\n",
      " [  7.97750287]\n",
      " [  9.40119784]\n",
      " [  9.33746718]\n",
      " [ 10.57824891]\n",
      " [ 13.96090134]\n",
      " [  8.53427856]\n",
      " [ 10.40064555]\n",
      " [ 10.44904033]\n",
      " [ 12.82108247]\n",
      " [  9.41684713]\n",
      " [ 10.05641039]\n",
      " [  9.63487953]\n",
      " [ 13.48989617]\n",
      " [  9.75017292]\n",
      " [  8.809516  ]\n",
      " [ 11.17310608]\n",
      " [ 10.86186337]\n",
      " [ 12.10912045]\n",
      " [ 10.80733336]\n",
      " [ 10.33398815]\n",
      " [ 10.88747931]\n",
      " [  9.10804981]\n",
      " [  8.78254431]\n",
      " [ 12.78252054]\n",
      " [ 10.92101009]\n",
      " [  9.54470004]\n",
      " [ 10.14025107]\n",
      " [  9.34431987]\n",
      " [  9.98540004]\n",
      " [  9.65618999]\n",
      " [ 11.24076272]\n",
      " [ 12.4371165 ]\n",
      " [  9.85894033]\n",
      " [ 11.45673511]\n",
      " [  8.37224137]\n",
      " [ 12.5404272 ]\n",
      " [ 11.24866969]\n",
      " [ 10.67345527]\n",
      " [  9.37588296]\n",
      " [ 11.81943367]\n",
      " [ 10.32077753]\n",
      " [ 13.57019556]\n",
      " [ 10.03471299]\n",
      " [  9.35093971]\n",
      " [ 10.0148034 ]\n",
      " [ 11.55964093]\n",
      " [ 10.73999597]\n",
      " [  9.90784154]\n",
      " [ 10.47746573]\n",
      " [  9.15537217]\n",
      " [  9.93295257]\n",
      " [  9.15611301]\n",
      " [  9.01137464]\n",
      " [ 13.35503626]\n",
      " [ 10.56926555]\n",
      " [ 10.1863659 ]\n",
      " [  8.84420797]\n",
      " [ 13.80099826]\n",
      " [ 10.95925174]\n",
      " [  9.74870217]\n",
      " [ 10.85464872]\n",
      " [ 12.04910201]\n",
      " [  9.79080714]\n",
      " [ 13.09095319]\n",
      " [ 10.17778809]\n",
      " [ 11.06338113]\n",
      " [ 10.52310559]\n",
      " [  9.77037857]\n",
      " [  9.8471001 ]\n",
      " [ 11.76289308]\n",
      " [ 11.1551014 ]\n",
      " [ 11.66499772]\n",
      " [  9.52944669]\n",
      " [ 11.72882811]\n",
      " [ 11.09695325]\n",
      " [  9.34298895]\n",
      " [ 10.22720095]\n",
      " [  9.67414787]\n",
      " [ 10.6940553 ]\n",
      " [ 11.66267606]\n",
      " [  9.43785218]\n",
      " [  9.28223404]\n",
      " [  9.12825886]\n",
      " [ 12.45115586]\n",
      " [ 11.26330412]\n",
      " [ 12.99508223]\n",
      " [ 12.15540547]\n",
      " [ 12.0419814 ]\n",
      " [  8.39565391]\n",
      " [ 12.09705866]\n",
      " [ 10.99916538]\n",
      " [ 10.11144473]\n",
      " [ 11.60146466]\n",
      " [ 10.08184698]\n",
      " [ 10.99199941]\n",
      " [ 10.65348334]\n",
      " [ 11.48004125]\n",
      " [ 10.42206468]\n",
      " [ 10.48845002]\n",
      " [  9.863673  ]\n",
      " [ 11.17432871]\n",
      " [ 10.02011055]\n",
      " [ 12.74600083]\n",
      " [ 12.27465119]\n",
      " [ 10.03369248]\n",
      " [ 10.58897226]\n",
      " [ 12.00867143]\n",
      " [  9.62632061]\n",
      " [ 10.23234963]\n",
      " [ 14.47232695]\n",
      " [ 12.89811482]\n",
      " [ 11.95242121]\n",
      " [  9.5087482 ]\n",
      " [  9.51399261]\n",
      " [ 10.18333739]\n",
      " [ 10.2533122 ]\n",
      " [  9.38323862]\n",
      " [ 11.25155051]\n",
      " [  8.47041824]\n",
      " [  7.66373426]\n",
      " [ 13.17925236]\n",
      " [  9.59315443]\n",
      " [ 10.14327528]\n",
      " [  9.8095081 ]\n",
      " [ 10.16094583]\n",
      " [  7.99138761]\n",
      " [ 10.06770482]\n",
      " [ 10.48374252]\n",
      " [  9.89907036]\n",
      " [ 12.74500452]\n",
      " [ 10.10336671]\n",
      " [ 10.8545472 ]\n",
      " [ 11.78607134]\n",
      " [  9.22482461]\n",
      " [ 13.52674732]\n",
      " [ 11.6183096 ]\n",
      " [ 10.8196339 ]\n",
      " [ 11.07182672]\n",
      " [  8.84033559]\n",
      " [  7.25952496]\n",
      " [  9.7086207 ]\n",
      " [ 12.36650314]\n",
      " [ 11.70299489]\n",
      " [ 11.35364591]\n",
      " [ 13.76070547]\n",
      " [ 10.9523126 ]\n",
      " [  9.52199286]\n",
      " [ 12.12898413]\n",
      " [ 10.96004741]\n",
      " [  9.65337512]\n",
      " [ 10.06170393]\n",
      " [ 10.34787428]\n",
      " [ 10.46002409]\n",
      " [ 11.2423702 ]\n",
      " [  9.6197464 ]\n",
      " [  9.77344207]\n",
      " [ 11.00380271]\n",
      " [ 11.54570597]\n",
      " [ 13.14566777]\n",
      " [ 11.87607721]\n",
      " [  9.87109014]\n",
      " [ 10.09386243]\n",
      " [  9.70643573]\n",
      " [ 10.30260954]\n",
      " [ 10.17700358]\n",
      " [  9.7242412 ]\n",
      " [  9.09798112]\n",
      " [ 10.42656376]\n",
      " [ 10.27487866]\n",
      " [ 10.31527848]\n",
      " [ 12.22348733]\n",
      " [ 12.09013008]\n",
      " [ 10.13907018]\n",
      " [ 12.88941945]\n",
      " [  9.14626134]\n",
      " [ 11.19409449]\n",
      " [ 13.68345482]\n",
      " [ 10.12751374]\n",
      " [ 12.09774935]\n",
      " [ 10.05778518]\n",
      " [ 10.71511739]\n",
      " [ 11.37671862]\n",
      " [ 13.07554127]\n",
      " [  9.56319362]\n",
      " [ 11.38258033]\n",
      " [ 10.04317939]\n",
      " [  9.73526102]\n",
      " [  9.2090405 ]\n",
      " [ 10.75797161]\n",
      " [ 12.83232332]\n",
      " [ 10.72423519]\n",
      " [ 10.11336874]\n",
      " [ 10.26209271]\n",
      " [ 12.13549589]\n",
      " [ 12.31682483]\n",
      " [  9.52301158]\n",
      " [ 10.52749728]\n",
      " [ 10.48332875]\n",
      " [ 10.75451267]\n",
      " [ 10.54726578]\n",
      " [  7.80020571]\n",
      " [  9.79071577]\n",
      " [ 12.8952171 ]\n",
      " [ 10.6629404 ]\n",
      " [ 10.07777432]\n",
      " [ 11.85841327]\n",
      " [ 10.72570464]\n",
      " [  9.53331824]\n",
      " [  9.36165984]\n",
      " [  9.46150901]\n",
      " [  9.80634606]\n",
      " [  9.74523983]\n",
      " [  8.64278128]\n",
      " [  9.13194683]\n",
      " [  8.26700554]\n",
      " [ 11.16872666]\n",
      " [ 10.41879682]\n",
      " [ 16.09538613]\n",
      " [ 12.16337976]\n",
      " [ 10.23656262]\n",
      " [ 12.82622462]\n",
      " [ 11.0581924 ]\n",
      " [ 10.70269656]\n",
      " [ 10.30171212]\n",
      " [ 10.30436672]\n",
      " [ 10.88039328]\n",
      " [ 12.3122441 ]\n",
      " [ 11.88261705]\n",
      " [ 12.11229708]\n",
      " [ 10.93190089]\n",
      " [ 11.54667618]\n",
      " [  9.93581634]\n",
      " [ 10.24704555]\n",
      " [  8.88956344]\n",
      " [ 11.17322896]\n",
      " [ 10.90585138]\n",
      " [  9.83079672]\n",
      " [  9.87961314]\n",
      " [  9.9565612 ]\n",
      " [ 10.50926475]\n",
      " [  9.96341857]\n",
      " [  9.50626412]\n",
      " [ 11.12282158]\n",
      " [  8.21565038]\n",
      " [ 13.95806079]\n",
      " [ 11.39387946]\n",
      " [ 11.85094608]\n",
      " [  9.43790255]\n",
      " [  8.52103666]\n",
      " [ 13.28773237]\n",
      " [  8.95229206]\n",
      " [ 10.1420374 ]\n",
      " [ 11.13387   ]\n",
      " [  9.69859779]\n",
      " [ 14.14966816]\n",
      " [ 10.25731969]\n",
      " [  9.90300471]\n",
      " [ 10.25461386]\n",
      " [ 10.88222533]\n",
      " [ 12.15186883]\n",
      " [  9.87823821]\n",
      " [ 11.62974237]\n",
      " [ 10.27415611]\n",
      " [ 11.53285576]\n",
      " [  8.45809678]\n",
      " [ 11.54418522]\n",
      " [ 13.34882671]\n",
      " [ 12.57555443]\n",
      " [ 10.16501971]\n",
      " [  9.77708565]\n",
      " [  9.64486778]\n",
      " [ 10.68541175]\n",
      " [ 10.46425713]\n",
      " [ 11.17528673]\n",
      " [ 11.0048063 ]\n",
      " [  9.48923151]\n",
      " [  8.85579134]\n",
      " [  9.28172014]\n",
      " [ 10.2948622 ]\n",
      " [ 12.28522588]\n",
      " [ 12.25214314]\n",
      " [ 10.51580752]\n",
      " [  9.32281489]\n",
      " [  9.92865236]\n",
      " [ 10.712233  ]\n",
      " [ 11.87994648]\n",
      " [ 10.56918623]\n",
      " [  9.80780144]\n",
      " [ 11.45860873]\n",
      " [ 12.77127039]\n",
      " [  8.97675444]\n",
      " [ 12.66560634]\n",
      " [  9.28899091]\n",
      " [ 10.77082434]\n",
      " [  9.51256184]\n",
      " [  9.71471938]\n",
      " [ 10.5703482 ]\n",
      " [ 12.34296742]\n",
      " [  8.63966313]\n",
      " [ 10.72772332]\n",
      " [ 10.28466411]\n",
      " [ 10.09916985]\n",
      " [  9.72968914]\n",
      " [ 11.17464589]\n",
      " [  9.24713561]\n",
      " [ 11.74517263]\n",
      " [ 11.70375459]\n",
      " [ 10.21128019]\n",
      " [ 10.60619274]\n",
      " [ 11.10527078]\n",
      " [ 11.1913281 ]\n",
      " [ 11.24674482]\n",
      " [  8.71522352]\n",
      " [ 10.66721661]\n",
      " [ 11.6844629 ]\n",
      " [ 12.50355576]\n",
      " [ 12.3462222 ]\n",
      " [ 11.78165426]\n",
      " [  8.72838992]\n",
      " [  9.44618773]\n",
      " [  9.69578737]\n",
      " [  9.68520734]\n",
      " [ 10.3586589 ]\n",
      " [  9.00346906]\n",
      " [ 12.03154667]\n",
      " [ 11.93366268]\n",
      " [ 10.90393757]\n",
      " [ 10.32135778]\n",
      " [  9.55752002]\n",
      " [ 10.05830906]\n",
      " [  8.58130265]\n",
      " [ 12.26298018]\n",
      " [  9.6285399 ]\n",
      " [ 10.70505413]\n",
      " [  9.7158111 ]\n",
      " [ 10.81908738]\n",
      " [ 11.35663974]\n",
      " [ 12.51393589]\n",
      " [ 12.54514493]\n",
      " [  8.02020679]\n",
      " [ 11.74177672]\n",
      " [ 10.26235008]\n",
      " [  9.65793936]\n",
      " [ 14.01202534]\n",
      " [  9.78938786]\n",
      " [  8.94524302]]\n",
      "r\n",
      "(500, 10)\n",
      "[[ 0.06249652  0.08639513  0.1025831  ...,  0.13125386  0.11043995\n",
      "   0.14395181]\n",
      " [ 0.06128203  0.16175266  0.05539988 ...,  0.16754453  0.11213471\n",
      "   0.11372191]\n",
      " [ 0.04259645  0.16298888  0.05426464 ...,  0.20846693  0.08073541\n",
      "   0.17311751]\n",
      " ..., \n",
      " [ 0.14244376  0.03438787  0.07918679 ...,  0.02775003  0.06124902\n",
      "   0.06109073]\n",
      " [ 0.09892771  0.08057823  0.0925596  ...,  0.08159579  0.09903949\n",
      "   0.12640183]\n",
      " [ 0.07764257  0.15407635  0.09163687 ...,  0.13805448  0.10471987\n",
      "   0.12081721]]\n",
      "exp_Syi\n",
      "(500,)\n",
      "[ 0.84853515  1.58604566  0.55034095  0.95557101  0.68195114  0.53484896\n",
      "  1.29364239  0.39676541  0.56904843  1.7635743   1.53711352  0.90790676\n",
      "  0.63014135  1.19579213  0.899293    0.65245248  1.19453898  1.39176749\n",
      "  1.48136435  0.81606428  1.12804072  0.94409192  4.48902262  1.11188652\n",
      "  1.55921379  0.53097061  0.37581612  0.83005387  0.65288462  0.98871962\n",
      "  1.21412181  1.30204728  1.16411093  0.68692898  0.58019149  1.11508373\n",
      "  1.10149816  0.91599205  1.05707061  0.79973902  1.00927043  0.51632401\n",
      "  0.78839567  1.47295607  0.43991079  1.17844747  0.6213356   1.10352073\n",
      "  1.21928349  1.03584849  0.69854272  0.98941823  0.81105599  0.99967779\n",
      "  0.99272959  1.33434252  0.96144027  1.98138648  2.80866214  0.67506592\n",
      "  1.31043547  0.4868499   0.81454796  0.78273241  0.7690038   0.98882779\n",
      "  1.10990287  0.52072479  1.27020114  1.0314279   0.69217069  1.76445161\n",
      "  0.75286354  0.88479412  0.63302054  0.69282501  1.68033582  1.22327977\n",
      "  1.13511103  0.86271351  0.7738949   0.68418549  0.74533135  1.40535718\n",
      "  1.05666602  1.04162925  0.9202891   0.79212583  0.35699439  3.27003634\n",
      "  0.60061139  0.66784708  1.03480223  0.87883643  1.22907945  1.27659329\n",
      "  0.7240948   1.09192887  0.83021714  0.60074673  0.8062919   0.89827713\n",
      "  0.67688539  1.11896168  0.62361392  0.80192557  0.82295908  0.59332978\n",
      "  0.83895914  1.16328819  1.20333027  1.68562043  1.15255226  0.71896563\n",
      "  0.97958816  0.63714031  1.38815586  0.74879846  0.78517952  0.88736324\n",
      "  0.77469561  0.49824098  0.80359575  1.21441929  1.2020791   0.6469869\n",
      "  1.08659391  1.41408699  1.31641578  1.40726214  1.51304844  1.27091538\n",
      "  1.27324074  1.11011974  0.94945353  1.64838876  1.00833967  1.16332687\n",
      "  1.06916592  1.01544218  1.10780283  1.32881834  0.93808497  1.50298095\n",
      "  1.61271266  1.50789896  0.91548245  1.21505258  1.48854456  0.98216222\n",
      "  0.89492358  0.70654523  0.91636891  1.19589568  0.63237998  1.52646152\n",
      "  1.24561394  0.60322374  1.26754514  0.58281733  1.27266846  1.0297575\n",
      "  1.07893732  0.86659047  0.58936916  0.75270416  1.34847556  0.8427312\n",
      "  0.86038734  0.68623212  1.14645714  1.13809685  0.92684024  0.91818214\n",
      "  1.34458093  0.74873532  1.25777286  0.81674454  1.07239855  0.86340316\n",
      "  0.95619705  0.70191289  0.67650849  0.62902489  1.28469809  2.64239528\n",
      "  1.34891525  1.44564663  0.95791684  1.1821122   1.43690446  1.46622235\n",
      "  0.88980378  1.27971292  1.0541135   1.06595146  1.00564289  0.86917629\n",
      "  1.06628701  1.25007576  0.57319659  1.36809349  0.58794105  0.47222491\n",
      "  1.18203363  0.97550112  0.58367322  1.1134142   1.02670687  1.06476729\n",
      "  0.99223777  0.53816518  1.13733327  0.88891168  1.25298577  1.20516627\n",
      "  0.76468501  1.16126393  1.11467805  0.83570355  1.18156658  1.03328698\n",
      "  0.47651926  1.05438217  0.9638784   1.65872455  0.65220025  1.54335829\n",
      "  1.17256098  1.44260754  0.86562608  1.05844251  1.83753368  1.27852376\n",
      "  1.25571596  0.83709847  1.02726167  0.67154678  0.93967966  1.10092611\n",
      "  0.79355243  1.27691481  0.60464756  1.06431069  0.78902258  1.01627444\n",
      "  1.17988387  0.9580369   1.12297869  1.21481644  1.52199442  0.96519932\n",
      "  1.18452764  0.84964228  0.98632828  0.4786656   0.83736207  0.50894527\n",
      "  1.0523467   0.89645134  1.92614688  1.31484538  1.03210105  0.43498996\n",
      "  1.62025553  0.95453188  0.56476708  1.01975302  0.90437869  1.2797312\n",
      "  1.18832125  1.54738523  0.64156     0.57410291  0.60934232  1.19296457\n",
      "  0.62421345  2.45802543  0.6383871   0.48433851  0.66778724  1.1120487\n",
      "  1.21073925  1.6816371   0.871476    1.20788762  1.40789481  0.9423111\n",
      "  1.58571965  1.76756872  1.21105969  0.97776272  0.99207908  0.66511244\n",
      "  0.99829997  2.16158281  0.77108138  1.35747154  1.0944763   1.13268531\n",
      "  1.15403852  0.68466614  0.90127276  1.29312044  0.60400087  0.63552183\n",
      "  0.65984983  0.88981851  0.5964635   0.42500847  1.20881603  1.54873886\n",
      "  1.47995942  0.99263376  0.64193072  1.16375103  0.94832901  0.93380086\n",
      "  0.64021125  1.33145484  0.5547268   1.15793735  0.94093624  1.09158366\n",
      "  1.31529531  1.3675914   0.94871278  1.11936328  0.65158842  0.43633514\n",
      "  1.31983021  0.82459399  0.3560349   1.36993346  0.75483637  1.20349056\n",
      "  1.1377514   1.13873822  0.61367785  1.15650786  1.06910309  0.82939649\n",
      "  0.95262382  0.93945927  1.67966422  1.12699843  1.05718181  0.98234007\n",
      "  1.38083631  0.66585025  1.02062771  0.7694697   1.03245998  0.88158256\n",
      "  0.66162092  1.0093371   1.5255342   0.63500032  0.98449595  1.50506482\n",
      "  0.61713203  0.76761388  1.39797281  0.73417679  0.57693092  1.0101022\n",
      "  0.78835844  0.88004412  0.57017339  1.25230217  1.28484396  1.4065524\n",
      "  1.72107122  1.8317058   1.18893902  0.97505811  0.41212908  1.08537569\n",
      "  1.64338992  0.99439301  1.21991554  0.69077428  2.19308055  1.4048825\n",
      "  1.39830063  0.88173526  0.86905274  0.72905682  1.81574272  1.0671717\n",
      "  0.63978036  0.71755043  0.96229228  0.82516344  0.85789918  0.90775464\n",
      "  0.92445935  0.49691522  0.83917031  1.04498409  0.89733496  0.87516355\n",
      "  0.66193265  0.93562787  0.58533995  1.01378477  1.48579086  0.95528327\n",
      "  1.98278815  0.71782511  0.85119338  1.28941608  0.74910824  3.45716625\n",
      "  1.06236369  0.94468041  1.38662604  1.0957425   0.84399675  1.45748429\n",
      "  1.26063768  0.54224402  0.96276211  0.62901358  1.11822379  0.74521554\n",
      "  0.72907425  1.66635914  1.74313633  1.05577654  0.85232298  0.64202458\n",
      "  0.86875349  0.92836717  0.93601893  1.20866242  0.98949032  0.6036851\n",
      "  1.33527866  1.7654896   1.26474093  0.86113169  2.01503265  1.32706187\n",
      "  1.10821677  1.45272317  0.74846034  1.2987091   0.68859199  0.64068\n",
      "  0.88197762  1.14165424  0.82287562  0.95525277  1.68280736  1.15613785\n",
      "  0.91194842  1.09007938  0.59300537  1.01318235  1.74478994  1.02041026\n",
      "  0.75833848  1.74756862  1.17390296  0.89467788  0.63667111  0.80842007\n",
      "  0.97306723  1.51017295  0.93854584  1.33283915  1.43208947  1.54010503\n",
      "  1.35387752  1.08479229  1.31776746  0.47044842  1.78454534  1.20839863\n",
      "  1.08141218  0.79294086  1.15596695  1.27600493  0.78133711  0.95340894\n",
      "  1.32667433  0.96563015  1.75555415  0.66744213  1.53227816  2.47731038\n",
      "  1.37562143  0.59734111  1.12922046  1.35963718  1.11478127  1.9959256\n",
      "  0.78881151  0.81971408]\n",
      "vectorized loss: 5.056010e+01 computed in 0.046763s\n",
      "Loss difference: 48.193770\n",
      "Gradient difference: 68248.290535\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized_soln\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved during cross-validation: -1.000000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
